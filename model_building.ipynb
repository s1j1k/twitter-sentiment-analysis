{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Building\n",
    "- implement classifiers using supervised and unsupervised models\n",
    "- analyze the classifer models with different parameters and input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from scipy.sparse import hstack\n",
    "#from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.util import accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# unsupervised \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# semisupervised\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "## importing libraries\n",
    "## data manipulation\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "## methods and stopwords text processing\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize # replace??\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Machine learning libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#NLP\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tweet preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering from raw tweets\n",
    " \n",
    "Provide a method to perform the preprocessing steps:\n",
    "1. casing\n",
    "2. noise removal\n",
    "3. tokenization\n",
    "4. stopword removal\n",
    "5. text normalization (stemming and lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def decode_str(str):\n",
    "    # fix encoding for emojis\n",
    "    soup = BeautifulSoup(str,\"html5lib\")\n",
    "    text = soup.text.encode('latin1').decode('utf-8','ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sally\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sally\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Citation (edited but sourced in part from):\n",
    "<Harshit Tyagi> (08/09/2020) <Sentiment_Analysis> (1) [.ipynb]. https://youtu.be/lMQzEk5vht4.\n",
    "\"\"\"\n",
    "\n",
    "## creating a stopwords set\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# required for preprocessing step\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    preprocess the text of the tweet\n",
    "    \"\"\"\n",
    "    # convert all text lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # remove any urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet,flags=re.MULTILINE)\n",
    "\n",
    "    # remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#',\"\",tweet)\n",
    "\n",
    "    # remove stop words\n",
    "    tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "\n",
    "    # lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrames for the different data formats\n",
    "- raw tweets\n",
    "- Tfidf tweets\n",
    "- embedded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_df(filepath):\n",
    "    with open(filepath,'rb') as f:\n",
    "        data = pickle.load(f)#,encoding='utf-8')    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "train_raw = pickle_to_df('./raw-tweets/tweets-data/train.pkl')\n",
    "dev_raw = pickle_to_df('./raw-tweets/tweets-data/dev.pkl')\n",
    "test_raw = pickle_to_df('./raw-tweets/tweets-data/test.pkl')\n",
    "unlabelled_raw = pickle_to_df('./raw-tweets/tweets-data/test.pkl')\n",
    "\n",
    "train_tfidf = pickle_to_df('./tfidf-tweets/tfidf/train_tfidf.pkl')\n",
    "dev_tfidf = pickle_to_df('./tfidf-tweets/tfidf/dev_tfidf.pkl')\n",
    "test_tfidf = pickle_to_df('./tfidf-tweets/tfidf/test_tfidf.pkl')\n",
    "unlabelled_tfidf = pickle_to_df('./tfidf-tweets/tfidf/unlabeled_tfidf.pkl')\n",
    "\n",
    "train_emb = pickle_to_df('./embedding-tweets/sentence-transformers/train_emb.pkl')\n",
    "dev_emb = pickle_to_df('./embedding-tweets/sentence-transformers/dev_emb.pkl')\n",
    "test_emb = pickle_to_df('./embedding-tweets/sentence-transformers/test_emb.pkl')\n",
    "unlabelled_emb = pickle_to_df('./embedding-tweets/sentence-transformers/unlabeled_emb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 4000, 4000, 4000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw), len(dev_raw), len(test_raw), len(unlabelled_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65    Bro ass be wolfin _TWITTER-ENTITY_ âbut that...\n",
       "98        _TWITTER-ENTITY_ give her a kissss for me â¤\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['text'][[65,98]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65    Bro ass be wolfin _TWITTER-ENTITY_ ✋but that's...\n",
       "98          _TWITTER-ENTITY_ give her a kissss for me ❤\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['text'].apply(decode_str)[[65,98]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression evaluation\n",
    "# note you should use the same evaluation for each model\n",
    "# todo tune evaluation parameters\n",
    "#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "\n",
    "# X_train_dev = text_transformer.fit_transform(pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]))\n",
    "# X_train = X_train_dev[:len(train_raw),:]\n",
    "# X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "# logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "# logit.fit(X_train, y_train)\n",
    "# y_pred = logit.predict(X_dev)\n",
    "# f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Supervised Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "- create a baseline logistic regression model \n",
    "- try it on different feature representations\n",
    "- check accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) # binary classification problem (positive/negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Tweets\n",
    "\n",
    "- try with common preprocessing method\n",
    "- try your own preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first decode the tweets and make readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_new = train_raw.copy()\n",
    "train_raw_new['text'] = train_raw_new['text'].apply(decode_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform the tweets with TFIDF (note: No text processing is done besides the decoding of text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# transform with TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- check f1 score\n",
    "- optimize\n",
    "- tune parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without decoding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dev = text_transformer.fit_transform(pd.concat([train_raw['text'],dev_raw['text']]))\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "X_dev = X_train_dev[len(train_raw):,:]\n",
    "y_train = train_raw['Sentiment']\n",
    "y_dev = dev_raw['Sentiment']\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try with pre processing `preprocess` and then `vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66775"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]).apply(preprocess)\n",
    "X_train_dev = vectorizer.fit_transform(preprocessed)\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefine the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strings = {\n",
    "    \"_TWITTER-ENTITY_\":\" \",\n",
    "    \"& amp ;\":\"and\",\n",
    "    \"& lt ;\":\"<\",\n",
    "    \"& gt ;\":\">\"\n",
    "}\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    preprocess the text of the tweet\n",
    "    \"\"\"\n",
    "    # replace the random strings\n",
    "    for x in replace_strings:\n",
    "        tweet = tweet.replace(x, replace_strings[x])\n",
    "\n",
    "    # remove any urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet,flags=re.MULTILINE)\n",
    "\n",
    "    # remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#|\\\\',\"\",tweet)\n",
    "\n",
    "    # ensure polarity of negations is captured\n",
    "\n",
    "    # exception in english\n",
    "    if \"can't\" in tweet.lower():\n",
    "        tweet = tweet.replace(\"can't\",\"can not\")\n",
    "        tweet = tweet.replace(\"CAN'T\",\"CAN NOT\")\n",
    "        tweet = re.sub(r'[Cc][Aa][Nn]\\'[tT]','can not',tweet)\n",
    "\n",
    "    # exception in english\n",
    "    if \"shan't\" in tweet.lower():\n",
    "        tweet = tweet.replace(\"shan't\",\"shall not\")\n",
    "        tweet = tweet.replace(\"SHAN'T\",\"SHALL NOT\")\n",
    "        tweet = re.sub(r'[Ss][Hh][Aa][Nn]\\'[tT]','shall not',tweet)\n",
    "\n",
    "    # should cover the rest of the contractions\n",
    "    if \"n't\" in tweet.lower():\n",
    "        tweet = tweet.replace(\"n't\",\" not\")\n",
    "        tweet = tweet.replace(\"N'T\",\" NOT\")\n",
    "        tweet = re.sub(r'[Nn]\\'[tT]',' not',tweet)\n",
    "\n",
    "    \n",
    "    # replace repeating letters more than 2 times \n",
    "    tweet = re.sub('^NO+', 'NO', tweet)\n",
    "    tweet = re.sub('^no+', 'no', tweet)\n",
    "    tweet = re.sub('^N[oO]+', 'No', tweet)\n",
    "    tweet = re.sub('^n[oO]+', 'no', tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67075"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]).apply(preprocess)\n",
    "X_train_dev = vectorizer.fit_transform(preprocessed)\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strings = {\n",
    "    \"_TWITTER-ENTITY_\":\"\",\n",
    "    \"& amp ;\":\"and\",\n",
    "    \"& lt ;\":\"<\",\n",
    "    \"& gt ;\":\">\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67075"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]).apply(preprocess)\n",
    "X_train_dev = vectorizer.fit_transform(preprocessed)\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that changing the replacement of user name to \"\" yields a different result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strings = {\n",
    "    \"_TWITTER-ENTITY_\":\" \",\n",
    "    \"& amp ;\":\"\",\n",
    "    \"& lt ;\":\"\",\n",
    "    \"& gt ;\":\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67075"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]).apply(preprocess)\n",
    "X_train_dev = vectorizer.fit_transform(preprocessed)\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_strings = {\n",
    "    \"_TWITTER-ENTITY_\":\" \",\n",
    "    \"& amp ;\":\"and\",\n",
    "    \"& lt ;\":\"<\",\n",
    "    \"& gt ;\":\">\"\n",
    "}\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    preprocess the text of the tweet\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # replace the random strings\n",
    "    for x in replace_strings:\n",
    "        tweet = tweet.replace(x, replace_strings[x])\n",
    "\n",
    "    # replace emojis with the meanings\n",
    "    tweet = emoji.demojize(tweet)\n",
    "\n",
    "    # remove any urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\",\"\",tweet,flags=re.MULTILINE)\n",
    "\n",
    "    # remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#|\\\\',\"\",tweet)\n",
    "\n",
    "    # ensure polarity of negations is captured\n",
    "\n",
    "    # exception in english\n",
    "    if \"can't\" in tweet.lower():\n",
    "        tweet = tweet.replace(\"can't\",\"can not\")\n",
    "        tweet = tweet.replace(\"CAN'T\",\"CAN NOT\")\n",
    "        tweet = re.sub(r'[Cc][Aa][Nn]\\'[tT]','can not',tweet)\n",
    "\n",
    "    # exception in english\n",
    "    if \"shan't\" in tweet.lower():\n",
    "        tweet = tweet.replace(\"shan't\",\"shall not\")\n",
    "        tweet = tweet.replace(\"SHAN'T\",\"SHALL NOT\")\n",
    "        tweet = re.sub(r'[Ss][Hh][Aa][Nn]\\'[tT]','shall not',tweet)\n",
    "\n",
    "    # should cover the rest of the contractions\n",
    "    if \"n't\" in tweet.lower():\n",
    "        tweet = tweet.replace(\"n't\",\" not\")\n",
    "        tweet = tweet.replace(\"N'T\",\" NOT\")\n",
    "        tweet = re.sub(r'[Nn]\\'[tT]',' not',tweet)\n",
    "\n",
    "    \n",
    "    # replace repeating letters more than 2 times \n",
    "    tweet = re.sub('^NO+', 'NO', tweet)\n",
    "    tweet = re.sub('^no+', 'no', tweet)\n",
    "    tweet = re.sub('^N[oO]+', 'No', tweet)\n",
    "    tweet = re.sub('^n[oO]+', 'no', tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67175"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]).apply(preprocess)\n",
    "X_train_dev = vectorizer.fit_transform(preprocessed)\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw and unpreprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_raw = pd.concat([train_raw, dev_raw])\n",
    "\n",
    "X = text_transformer.fit_transform(pd.concat([train_dev_raw['text'],test_raw['text'],unlabelled_raw['text']]))\n",
    "\n",
    "X_train = X[:len(train_dev_raw),:]\n",
    "y_train = train_dev_raw['Sentiment']\n",
    "\n",
    "X_test = X[len(train_dev_raw):len(train_dev_raw)+len(test_raw),:]\n",
    "\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kaggle upload \n",
    "test_preds = y_pred\n",
    "test_preds = logit.predict(X_test)\n",
    "test_preds_df = pd.DataFrame({'Category':test_preds})\n",
    "test_preds_df.index.name = 'Id'\n",
    "test_preds_df.to_csv('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle result:\n",
    "\n",
    "0.67833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Demographic', ylabel='Count'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(test_raw.Demographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Demographic\n",
       "AAE    20000\n",
       "SAE    20000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.groupby(['Demographic']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dev = text_transformer.fit_transform(pd.concat([train_raw['text'],dev_raw['text'],unlabelled_raw['text'],test_raw['text']]))\n",
    "X_train = X_train_dev[:len(train_raw),:]\n",
    "y_train = train_raw['Sentiment']\n",
    "X_dev = X_train_dev[len(train_raw):len(train_raw)+len(dev_raw),:]\n",
    "\n",
    "X_train_AAE = X_train[train_raw['Demographic'] == 'AAE']\n",
    "y_train_AAE = train_raw[train_raw['Demographic'] == 'AAE']['Sentiment']\n",
    "\n",
    "X_train_SAE = X_train[train_raw['Demographic'] == 'SAE']\n",
    "y_train_SAE = train_raw[train_raw['Demographic'] == 'SAE']['Sentiment']\n",
    "\n",
    "X_dev_AAE = X_dev[dev_raw['Demographic'] == 'AAE']\n",
    "y_dev_AAE = dev_raw[dev_raw['Demographic'] == 'AAE']['Sentiment']\n",
    "\n",
    "X_dev_SAE = X_dev[dev_raw['Demographic'] == 'SAE']\n",
    "y_dev_SAE = dev_raw[dev_raw['Demographic'] == 'SAE']['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test training the model only on AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.637"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train_AAE, y_train_AAE)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test training the model only on SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66075"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train_SAE, y_train_SAE)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score decreases for both, when compared to training on both demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67475"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test making predictions only on AAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev_AAE)\n",
    "f1_score(y_dev_AAE, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test making predictions only on SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7154999999999999"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev_SAE)\n",
    "f1_score(y_dev_SAE, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_tfidf['TFIDF'].tolist()\n",
    "X_dev = dev_tfidf['TFIDF'].tolist()\n",
    "y_train = train_tfidf['Sentiment']\n",
    "y_dev = dev_tfidf['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67675"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_emb['TFIDF'].tolist()\n",
    "X_dev = dev_emb['TFIDF'].tolist()\n",
    "y_train = train_emb['Sentiment']\n",
    "y_dev = dev_emb['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69825"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best result so far on the embedded data set, faster than TFIDF, slower than raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_emb = pd.concat([train_emb, dev_emb])\n",
    "X_train = train_dev_emb.TFIDF.tolist()\n",
    "y_train = train_dev_emb['Sentiment']\n",
    "X_test = test_emb.TFIDF.tolist()\n",
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4) \n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_test)\n",
    "# for kaggle upload \n",
    "test_preds = y_pred\n",
    "test_preds = logit.predict(X_test)\n",
    "test_preds_df = pd.DataFrame({'Category':test_preds})\n",
    "test_preds_df.index.name = 'Id'\n",
    "test_preds_df.to_csv('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle result:\n",
    "\n",
    "0.66000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental result was higher for raw tweets not embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, word embeddings are the most effective feature representations for Logistic regression model\n",
    "\n",
    "Questions:\n",
    "- Is the model responsible for the low accuracy?\n",
    "- Is the choice of feature representation causing low accuracy?\n",
    "\n",
    "Need to test another model!\n",
    "\n",
    "In general for tweets, a model that is supervised is not practical.\n",
    "There is simply too many tweets and rapidly changing language convention for the learning to be effective and therefore unsupervised learning is more practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multinomial Naive Bayes\n",
    "- Python 3 Text Processing with NLTK 3 Cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the tfidf features as they are nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6755"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_tfidf['TFIDF'].tolist()\n",
    "y_train = train_tfidf['Sentiment']\n",
    "X_dev = dev_tfidf['TFIDF'].tolist()\n",
    "y_dev = dev_tfidf['Sentiment'].tolist()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try normalizing the features: Raw, TFIDF, embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6735"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "p.fit(X_train,y_train) \n",
    "p.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to use embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61425"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_emb['TFIDF'].tolist()\n",
    "y_train = train_emb['Sentiment']\n",
    "X_dev = dev_emb['TFIDF'].tolist()\n",
    "y_dev = dev_emb['Sentiment'].tolist()\n",
    "p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "p.fit(X_train,y_train) \n",
    "p.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network\n",
    "- multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68875"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_emb['TFIDF'].tolist()\n",
    "y_train = train_emb['Sentiment']\n",
    "X_dev = dev_emb['TFIDF'].tolist()\n",
    "y_dev = dev_emb['Sentiment']\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6905"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try with minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize the MLP\n",
    "- number of hidden layers\n",
    "- iterations/criteria\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try a different activation function\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "\n",
    "‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65975"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(activation='tanh', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.666"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(activation='logistic', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default activation function works best: \n",
    "\n",
    "‘relu’, the rectified linear unit function, returns f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does increasing iterations help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.691"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default max_iter = 200\n",
    "clf = MLPClassifier(max_iter=500, solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes it does, what about increasing it even more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6895"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(max_iter=5000, solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize the layers,k use an empirical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65725"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69075"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68325"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 4, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6705"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 7, 4, 2), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6865"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(9, 3, 4), random_state=1)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_dev_scaled)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the neural network parameters\n",
    "- learning rate\n",
    "- layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try it on the tfidf data since feature engineering ntoe required for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67875"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_tfidf['TFIDF'].tolist()\n",
    "y_train = train_tfidf['Sentiment']\n",
    "X_dev = dev_tfidf['TFIDF'].tolist()\n",
    "y_dev = dev_tfidf['Sentiment']\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 4, 2), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Unsupervised Learners\n",
    "\n",
    "- VADER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "\n",
    "The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).\n",
    "positive sentiment : (compound score >= 0.05) \n",
    "neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "negative sentiment : (compound score <= -0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^ todo try using a higher weight for AAE to stablize bias (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised import UnsupervisedClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', ..., 'positive', 'positive',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = UnsupervisedClassifier()\n",
    "#clf.fit(train_raw['text'], train_emb['TFIDF'])\n",
    "# testing, reduce the data size \n",
    "clf.fit(train_raw['text'].iloc[:50], train_emb['TFIDF'].iloc[:50])\n",
    "clf.predict(dev_raw['text'], dev_emb['TFIDF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the classifier takes series input\n",
    "X_train_raw = train_raw['text']\n",
    "X_train_emb = train_emb['TFIDF']\n",
    "\n",
    "X_dev_raw = dev_raw['text']\n",
    "X_dev_emb = dev_emb['TFIDF']\n",
    "y_dev = dev_emb['Sentiment']\n",
    "\n",
    "\n",
    "clf = UnsupervisedClassifier()\n",
    "\n",
    "clf.fit(X_train_raw, X_train_emb)\n",
    "y_pred = clf.predict(X_dev_raw, X_dev_emb)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increase the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the classifier takes series input\n",
    "X_train_raw = train_raw['text']\n",
    "X_train_emb = train_emb['TFIDF']\n",
    "\n",
    "X_dev_raw = dev_raw['text']\n",
    "X_dev_emb = dev_emb['TFIDF']\n",
    "y_dev = dev_emb['Sentiment']\n",
    "\n",
    "\n",
    "clf = UnsupervisedClassifier(10)\n",
    "\n",
    "clf.fit(X_train_raw, X_train_emb)\n",
    "y_pred = clf.predict(X_dev_raw, X_dev_emb)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier takes series input\n",
    "X_train_raw = train_raw['text']\n",
    "X_train_emb = train_emb['TFIDF']\n",
    "\n",
    "X_dev_raw = dev_raw['text']\n",
    "X_dev_emb = dev_emb['TFIDF']\n",
    "y_dev = dev_emb['Sentiment']\n",
    "\n",
    "\n",
    "clf = UnsupervisedClassifier(mode='simple')\n",
    "\n",
    "clf.fit(X_train_raw, X_train_emb)\n",
    "y_pred = clf.predict(X_dev_raw, X_dev_emb)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training data has been fully predicted (without supervision)\n",
    "\n",
    "Predict labels of the dev set and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neutral sentiments, use KMeans clustering via embedded features\n",
    "\n",
    "Feature embedding is pre-trained, is numeric and gives meaning according to distance in 3D space\n",
    "\n",
    "Todo normalize Embeddings (???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semisupervised learner/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline semi-supervised learner\n",
    "- train the model on training and dev data\n",
    "- make predictions for unlabelled data\n",
    "- cross-validate the model with all of the x,y from training, dev, and unlabelled data and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering, majority voting, 1NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive' 'negative' ... 'positive' 'positive' 'negative'] ['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.52575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a kaggle submission from this classifier\n",
    "from semisupervised import KMeansClustering1NN\n",
    "clf = KMeansClustering1NN(2)\n",
    "\n",
    "# train it on train_val and also on unlabelled data \n",
    "# see if it performs better than just KMeans alone!\n",
    "X_train = np.array(train_emb['TFIDF'].tolist() + unlabelled_emb['TFIDF'].tolist())[:10]\n",
    "y_train = np.array(train_emb['Sentiment'].tolist() + ['None'] * len(unlabelled_emb))[:10]\n",
    "\n",
    "X_dev = np.array(dev_emb['TFIDF'].tolist())\n",
    "y_dev = np.array(dev_emb['Sentiment'].tolist())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_dev)\n",
    "\n",
    "print(y_dev, y_pred)\n",
    "\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive' 'negative' ... 'positive' 'positive' 'negative'] ['positive' 'positive' 'negative' ... 'negative' 'positive' 'negative']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.52575"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a kaggle submission from this classifier\n",
    "from semisupervised import KMeansClustering1NN\n",
    "clf = KMeansClustering1NN(10)\n",
    "\n",
    "# train it on train_val and also on unlabelled data \n",
    "# see if it performs better than just KMeans alone!\n",
    "X_train = np.array(train_emb['TFIDF'].tolist() + unlabelled_emb['TFIDF'].tolist())[:10]\n",
    "y_train = np.array(train_emb['Sentiment'].tolist() + ['None'] * len(unlabelled_emb))[:10]\n",
    "\n",
    "X_dev = np.array(dev_emb['TFIDF'].tolist())\n",
    "y_dev = np.array(dev_emb['Sentiment'].tolist())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_dev)\n",
    "\n",
    "print(y_dev, y_pred)\n",
    "\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a kaggle submission from this classifier\n",
    "from semisupervised import KMeansClustering1NN\n",
    "clf = KMeansClustering1NN(2)\n",
    "\n",
    "# train it on train_val and also on unlabelled data \n",
    "# see if it performs better than just KMeans alone!\n",
    "X_train = np.array(train_val_emb['TFIDF'].tolist() + unlabeled_emb['TFIDF'].tolist())\n",
    "y_train = np.array(train_val_emb['Sentiment'].tolist() + ['None'] * len(unlabeled_emb))\n",
    "\n",
    "X_test = np.array(test_emb['TFIDF'].tolist())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle result:\n",
    "\n",
    "0.61000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category\n",
       "Id            \n",
       "0     negative\n",
       "1     positive\n",
       "2     positive\n",
       "3     negative\n",
       "4     negative\n",
       "...        ...\n",
       "3995  negative\n",
       "3996  positive\n",
       "3997  negative\n",
       "3998  negative\n",
       "3999  positive\n",
       "\n",
       "[4000 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try with more clusters\n",
    "# prepare a kaggle submission from this classifier\n",
    "from semisupervised import KMeansClustering1NN\n",
    "clf = KMeansClustering1NN(10)\n",
    "\n",
    "# train it on train_val and also on unlabelled data \n",
    "# see if it performs better than just KMeans alone!\n",
    "X_train = np.array(train_val_emb['TFIDF'].tolist() + unlabeled_emb['TFIDF'].tolist())\n",
    "y_train = np.array(train_val_emb['Sentiment'].tolist() + ['None'] * len(unlabeled_emb))\n",
    "\n",
    "X_test = np.array(test_emb['TFIDF'].tolist())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# for kaggle upload \n",
    "test_preds = y_pred\n",
    "test_preds_df = pd.DataFrame({'Category':test_preds})\n",
    "test_preds_df.index.name = 'Id'\n",
    "test_preds_df.to_csv('test_preds.csv')\n",
    "test_preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle result with more clusters (10):\n",
    "\n",
    "Score: 0.62749"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelfTrainingClassifier \n",
    "- logistic regression\n",
    "- support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='ovr', random_state=17, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_emb['TFIDF'].tolist()\n",
    "X_dev = dev_emb['TFIDF'].tolist()\n",
    "y_train = train_emb['Sentiment']\n",
    "y_dev = dev_emb['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test it once without unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SelfTrainingClassifier(logit)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([train_emb, unlabelled_emb]).TFIDF.tolist()\n",
    "y_train = pd.concat([train_emb, unlabelled_emb]).Sentiment.apply(lambda x: x if x != None else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with the unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SelfTrainingClassifier(logit)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semisupervised MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_emb['TFIDF'].tolist()\n",
    "X_dev = dev_emb['TFIDF'].tolist()\n",
    "y_train = train_emb['Sentiment']\n",
    "y_dev = dev_emb['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6925"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Semisupvervised MLP\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 4, 2), random_state=1)\n",
    "#clf = SelfTrainingClassifier(mlp)\n",
    "clf = SelfTrainingClassifier(mlp, criterion='k_best', k_best=20)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([train_emb, unlabelled_emb]).TFIDF.tolist()\n",
    "y_train = pd.concat([train_emb, unlabelled_emb]).Sentiment.apply(lambda x: x if x != None else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Semisupvervised MLP\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 4, 2), random_state=1)\n",
    "#clf = SelfTrainingClassifier(mlp)\n",
    "clf = SelfTrainingClassifier(mlp, criterion='k_best', k_best=20, max_iter=2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-7f00be271c99>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Semisupvervised MLP\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmlp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMLPClassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msolver\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'lbfgs'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1e-5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_layer_sizes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m7\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;31m#clf = SelfTrainingClassifier(mlp)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mclf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSelfTrainingClassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmlp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'k_best'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk_best\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_iter\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mclf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'MLPClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Semisupvervised MLP\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 4, 2), random_state=1)\n",
    "#clf = SelfTrainingClassifier(mlp)\n",
    "clf = SelfTrainingClassifier(mlp, criterion='k_best', k_best=20, max_iter=2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "f1_score(y_dev, y_pred, average='micro')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b1f6a4c94cc67db81c14d05ec8082b932fd058d531346646808c426a47add8f"
  },
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
